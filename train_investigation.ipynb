{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-01T10:48:34.118485Z",
     "start_time": "2017-10-01T10:48:33.799201Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import functools\n",
    "import math\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-01T10:48:39.596393Z",
     "start_time": "2017-10-01T10:48:35.692213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49592205\n",
      "49591950\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('./en_train.csv', sep=',', encoding='utf8',\n",
    "    dtype={'sentence_id': np.int32, 'token_id': np.int32, 'class': unicode, 'before': unicode, 'after': unicode})\n",
    "print train.size\n",
    "train.dropna(axis=0, how='any', inplace=True)\n",
    "print train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['before'] = train.before.astype(unicode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9918390 entries, 0 to 9918440\n",
      "Data columns (total 5 columns):\n",
      "sentence_id    int32\n",
      "token_id       int32\n",
      "class          object\n",
      "before         object\n",
      "after          object\n",
      "dtypes: int32(2), object(3)\n",
      "memory usage: 378.4+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-01T19:51:50.926935Z",
     "start_time": "2017-10-01T19:51:50.746294Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "class LabelClassifier:\n",
    "\n",
    "    def __init__(self, data, num_hidden=64, seq_len=100, batch_size=1000, num_layers=3):\n",
    "        self._data = data\n",
    "        self._num_hidden = num_hidden\n",
    "        self._seq_len = seq_len\n",
    "        self._batch_size = batch_size\n",
    "        self._num_layers = num_layers\n",
    "        self._learning_rate = .003\n",
    "        self._max_grad_norm = .5\n",
    "        self._embedding_size = 32\n",
    "\n",
    "        self._init_dict()\n",
    "        self._init_classes()\n",
    "\n",
    "        self._input = tf.placeholder(tf.float32, [None, self._seq_len, self._embedding_size], name='input')\n",
    "        self._target = tf.placeholder(tf.float32, [None, self._num_classes], name='target')\n",
    "\n",
    "        self._init_length()\n",
    "        self._init_prediction()\n",
    "        self._init_cost()\n",
    "        self._init_error()\n",
    "        self._init_optimize()\n",
    "\n",
    "    def _init_dict(self):\n",
    "        self._vocab_size = 0\n",
    "        self._char_to_id = {}\n",
    "        self._char_to_id[u'\\u0000'] = 0\n",
    "        for text in self._data['before']:\n",
    "            for c in text:\n",
    "                if c not in self._char_to_id:\n",
    "                    id = len(self._char_to_id)\n",
    "                    self._char_to_id[c] = id\n",
    "        self._vocab_size = len(self._char_to_id)\n",
    "        self._embeddings = np.random.random([self._vocab_size, self._embedding_size])\n",
    "        print 'vocabulary size: ', self._vocab_size\n",
    "\n",
    "\n",
    "    def _init_classes(self):\n",
    "        self._class_to_one_hot = {}\n",
    "        self._num_classes = 0\n",
    "        class_list = self._data['class'].unique()\n",
    "        class_one_hots = np.eye(len(class_list))\n",
    "        for i in range(len(class_one_hots)):\n",
    "            self._class_to_one_hot[class_list[i]] = class_one_hots[i,:]\n",
    "        self._num_classes = len(class_one_hots)\n",
    "        print 'number of classed: ', self._num_classes\n",
    "\n",
    "\n",
    "    def _init_length(self):\n",
    "        used = tf.sign(tf.reduce_max(tf.abs(self._input), reduction_indices=2))\n",
    "        length = tf.reduce_sum(used, reduction_indices=1)\n",
    "        self._length = tf.cast(length, tf.int32)\n",
    "\n",
    "\n",
    "    def _init_prediction(self):\n",
    "        # Dimensions\n",
    "        self._max_length = int(self._input.get_shape()[1])\n",
    "        batch_size = tf.shape(self._input)[0]\n",
    "\n",
    "        # Recurrent network\n",
    "        with tf.variable_scope('rnn'):\n",
    "            cells = []\n",
    "            for _ in range(self._num_layers):\n",
    "                cells.append(tf.contrib.rnn.GRUCell(self._num_hidden))\n",
    "            cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "            states = cell.zero_state(batch_size, tf.float32)\n",
    "            state_type = type(states)\n",
    "            self._initial_state = [\n",
    "                tf.placeholder_with_default(zero_state, [None, self._num_hidden]) for zero_state in states]\n",
    "            self._initial_state = state_type(self._initial_state)\n",
    "\n",
    "            self._output, self._final_state = tf.nn.dynamic_rnn(cell, self._input,\n",
    "                                                                dtype=tf.float32, sequence_length=self._length,\n",
    "                                                                initial_state=self._initial_state)\n",
    "\n",
    "        # Get relevant output\n",
    "        index = tf.range(0, batch_size) * self._max_length + (self._length - 1)\n",
    "        flat = tf.reshape(self._output, [-1, self._num_hidden])\n",
    "        relevant = tf.gather(flat, index)\n",
    "\n",
    "        # Softmax layer\n",
    "        with tf.variable_scope('softmax'):\n",
    "            weight = tf.get_variable('W', [self._num_hidden, self._num_classes])\n",
    "            bias = tf.get_variable('b', [self._num_classes], initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "            self._logits = tf.matmul(relevant, weight) + bias\n",
    "            self._prediction = tf.nn.softmax(self._logits)\n",
    "\n",
    "        # Summarize weigths\n",
    "        tf.summary.histogram(\"rnn_output\", self._output)\n",
    "        tf.summary.histogram(\"softmax_weights\", weight)\n",
    "        tf.summary.histogram(\"softmax_bias\", bias)\n",
    "        tf.summary.histogram(\"prediction\", self._prediction)\n",
    "\n",
    "        \n",
    "    def _init_cost(self):\n",
    "        with tf.variable_scope('cost'):\n",
    "            self._cost = tf.losses.softmax_cross_entropy(self._target, self._logits)\n",
    "        tf.summary.scalar('cost', self._cost)\n",
    "\n",
    "\n",
    "    def _init_error(self):\n",
    "        with tf.variable_scope('error'):\n",
    "            mistakes = tf.not_equal(tf.argmax(self._target, 1), tf.argmax(self._prediction, 1))\n",
    "            self._error = tf.reduce_mean(tf.cast(mistakes, dtype=tf.float32))\n",
    "\n",
    "        tf.summary.scalar('error', self._error)\n",
    "\n",
    "        \n",
    "    def _init_optimize(self):\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads = tf.gradients(self._cost, tvars)\n",
    "        clip_grads, _ = tf.clip_by_global_norm(grads, self._max_grad_norm)\n",
    "        optimizer = tf.train.AdamOptimizer(self._learning_rate)\n",
    "\n",
    "        self._optimize = optimizer.apply_gradients(zip(clip_grads, tvars))\n",
    "\n",
    "\n",
    "    def _get_batch(self, data):\n",
    "        sample = data.sample(n=self._batch_size)\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(len(sample)):\n",
    "            label = sample.iloc[i]['class']\n",
    "            y.append(self._class_to_one_hot[label])\n",
    "\n",
    "            text = sample.iloc[i]['before']\n",
    "            seq = [[0] * self._embedding_size for _ in range(self._seq_len)]\n",
    "            index = 0\n",
    "            if len(text) > self._seq_len:\n",
    "                text = text[:self._seq_len]\n",
    "\n",
    "            for c in text:\n",
    "                if c not in self._char_to_id:\n",
    "                    raise Exception('unknown symbol: ', c, ' in text: ', text)\n",
    "                seq[index] = self._embeddings[self._char_to_id[c]]\n",
    "                index += 1\n",
    "            x.append(np.array(seq))\n",
    "        return (np.array(x), np.array(y))\n",
    "    \n",
    "    \n",
    "    def train(self, log_dir, model_dir, num_steps, train, validation):\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        train_writer = tf.summary.FileWriter(log_dir + '/train', sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(log_dir + '/test', sess.graph)\n",
    "        summaries = tf.summary.merge_all()\n",
    "\n",
    "        train_writer.add_graph(sess.graph)\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        error = None\n",
    "        for step in range(num_steps):\n",
    "            x, y = self._get_batch(train)\n",
    "            _, s = sess.run([self._optimize, summaries], feed_dict={self._input: x, self._target: y})\n",
    "            train_writer.add_summary(s, step)\n",
    "\n",
    "            x, y = self._get_batch(validation)\n",
    "            s, e = sess.run([summaries, self._error], feed_dict={self._input: x, self._target: y})\n",
    "            test_writer.add_summary(s, step)\n",
    "            sys.stdout.write('\\r{0} {1}'.format(step, e))\n",
    "            if error is None or error > e:\n",
    "                saver.save(sess, model_dir + '/best.chkp')\n",
    "                error = e\n",
    "                print 'save best model for: ', error\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-01T19:55:23.601953Z",
     "start_time": "2017-10-01T19:51:52.018766Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size:  3082\n",
      "number of classed:  16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.254000008106save best model for:  0.254\n",
      "2 0.233999997377save best model for:  0.234\n",
      "12 0.232999995351save best model for:  0.233\n",
      "24 0.0939999967813save best model for:  0.094\n",
      "25 0.0920000001788save best model for:  0.092\n",
      "26 0.0729999989271save best model for:  0.073\n",
      "51 0.070000000298save best model for:  0.07\n",
      "90 0.0680000036955save best model for:  0.068\n",
      "91 0.0590000003576save best model for:  0.059\n",
      "132 0.0500000007451save best model for:  0.05\n",
      "138 0.0430000014603save best model for:  0.043\n",
      "139 0.0359999984503save best model for:  0.036\n",
      "144 0.0340000018477save best model for:  0.034\n",
      "163 0.0320000015199save best model for:  0.032\n",
      "188 0.0299999993294save best model for:  0.03\n",
      "220 0.0260000005364save best model for:  0.026\n",
      "245 0.0250000003725save best model for:  0.025\n",
      "254 0.0240000002086save best model for:  0.024\n",
      "258 0.0189999993891save best model for:  0.019\n",
      "296 0.0179999992251save best model for:  0.018\n",
      "311 0.0140000004321save best model for:  0.014\n",
      "377 0.00800000037998save best model for:  0.008\n",
      "681 0.00700000021607save best model for:  0.007\n",
      "800 0.00400000018999save best model for:  0.004\n",
      "1012 0.00300000002608save best model for:  0.003\n",
      "1988 0.00200000009499save best model for:  0.002\n",
      "3152 0.0010000000475save best model for:  0.001\n",
      "3248 0.0save best model for:  0.0\n",
      "9999 0.00700000021607"
     ]
    }
   ],
   "source": [
    "!rm -rf '/tmp/label_classification'\n",
    "\n",
    "model = LabelClassifier(train)\n",
    "model.train('/tmp/label_classification', './label_classifier', 10000, train[:800000], train[800000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "程\n"
     ]
    }
   ],
   "source": [
    "print u'\\u7a0b' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "u'\\u7a0b'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-2e34dadc0b15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_char_to_one_hot\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mu'\\u7a0b'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: u'\\u7a0b'"
     ]
    }
   ],
   "source": [
    "print model._char_to_one_hot[u'\\u7a0b']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-01T10:48:11.359098Z",
     "start_time": "2017-10-01T10:48:11.336245Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2d53abcdb2a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m616100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m616120\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train.iloc[616100:616120]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
