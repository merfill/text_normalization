{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-07T20:38:45.309669Z",
     "start_time": "2017-10-07T20:38:45.303251Z"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from char_extractor import CharExtractor\n",
    "from label_classifier import LabelClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-07T20:38:58.268220Z",
     "start_time": "2017-10-07T20:38:47.676625Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9918390 entries, 0 to 9918440\n",
      "Data columns (total 5 columns):\n",
      "sentence_id    int64\n",
      "token_id       int64\n",
      "class          object\n",
      "before         object\n",
      "after          object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 454.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('./en_train.csv', sep=',', encoding='utf8')\n",
    "train.dropna(axis=0, how='any', inplace=True)\n",
    "train['before'] = train.before.astype(unicode)\n",
    "train['after'] = train.after.astype(unicode)\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "char_extractor = CharExtractor.load_from_file('./lc_params.json')\n",
    "validation_data = train['class'].values\n",
    "input_data = train['before'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./lc_classifier/best.chkp\n",
      "number of iterations:  3\n",
      "processing batch:  0\n",
      "processing batch:  1\n",
      "processing batch:  2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "model = LabelClassifier(char_extractor._embedding_size, char_extractor._embeddings, char_extractor._char_to_id,\n",
    "                        char_extractor._start_id, char_extractor._end_id, char_extractor._label_one_hot,\n",
    "                        len(char_extractor._label_one_hot), char_extractor._id_to_label,\n",
    "                        char_extractor._label_to_id, state_size=256, num_layers=8)\n",
    "model.save_params('./lc_model.json')\n",
    "\n",
    "result = model.classify('./lc_classifier', input_data[:30000], 10000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259\n"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "for i in range(len(result)):\n",
    "    if result[i] != validation_data[i]:\n",
    "        errors.append((input_data[i], validation_data[i], result[i]))\n",
    "\n",
    "print len(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ALCS\" >> LETTERS = PLAIN\n",
      "\"-\" >> VERBATIM = PLAIN\n",
      "\"LL\" >> PLAIN = LETTERS\n",
      "\"Aceh\" >> LETTERS = PLAIN\n",
      "\"Aceh\" >> LETTERS = PLAIN\n",
      "\"Lviv\" >> LETTERS = PLAIN\n",
      "\"Ukh\" >> LETTERS = PLAIN\n",
      "\"FRELIMO\" >> PLAIN = LETTERS\n",
      "\"POMZ\" >> LETTERS = PLAIN\n",
      "\"Boev\" >> LETTERS = PLAIN\n",
      "\"-\" >> VERBATIM = PLAIN\n",
      "\"25-08-2015\" >> DATE = TELEPHONE\n",
      "\"POLARCAT\" >> LETTERS = PLAIN\n",
      "\"M\" >> LETTERS = PLAIN\n",
      "\"DoD\" >> LETTERS = PLAIN\n",
      "\"Revd\" >> LETTERS = PLAIN\n",
      "\"VIDEO\" >> PLAIN = LETTERS\n",
      "\"-\" >> VERBATIM = PLAIN\n",
      "\"OL\" >> LETTERS = PLAIN\n",
      "\"MUV\" >> LETTERS = PLAIN\n",
      "\"SUV\" >> LETTERS = PLAIN\n",
      "\"UA\" >> PLAIN = LETTERS\n",
      "\"CO\" >> LETTERS = VERBATIM\n",
      "\"DOH\" >> PLAIN = LETTERS\n",
      "\"SHU\" >> PLAIN = LETTERS\n",
      "\"F\" >> LETTERS = PLAIN\n",
      "\"200\" >> DIGIT = CARDINAL\n",
      "\"X\" >> LETTERS = PLAIN\n",
      "\"PIALAT\" >> LETTERS = PLAIN\n",
      "\"GETS\" >> PLAIN = LETTERS\n",
      "\"b\" >> LETTERS = PLAIN\n",
      "\"8 a.m.\" >> TIME = DATE\n",
      "\"XIX\" >> CARDINAL = PLAIN\n",
      "\"G\" >> PLAIN = LETTERS\n",
      "\"-\" >> VERBATIM = PLAIN\n",
      "\"-\" >> VERBATIM = PLAIN\n",
      "\"Wim\" >> LETTERS = PLAIN\n",
      "\"INTELLIGENCE\" >> PLAIN = LETTERS\n",
      "\"-\" >> VERBATIM = PLAIN\n",
      "\"747\" >> DIGIT = CARDINAL\n",
      "\"T\" >> LETTERS = PLAIN\n",
      "\"1/2007\" >> FRACTION = DATE\n",
      "\"1917\" >> CARDINAL = DATE\n",
      "\"-\" >> PUNCT = PLAIN\n",
      "\"1918\" >> CARDINAL = DATE\n",
      "\"Wim\" >> LETTERS = PLAIN\n",
      "\"K\" >> PLAIN = VERBATIM\n",
      "\"K\" >> PLAIN = VERBATIM\n",
      "\"seq\" >> LETTERS = PLAIN\n",
      "\"Pnyx\" >> LETTERS = PLAIN\n",
      "\"DAFIF\" >> LETTERS = PLAIN\n",
      "\"δ\" >> VERBATIM = PLAIN\n",
      "\"I\" >> CARDINAL = PLAIN\n",
      "\"2012\" >> CARDINAL = DATE\n",
      "\"-\" >> PUNCT = PLAIN\n",
      "\"V.\" >> ORDINAL = LETTERS\n",
      "\"-\" >> VERBATIM = PLAIN\n",
      "\"UMIST\" >> LETTERS = PLAIN\n",
      "\"GRIN\" >> PLAIN = LETTERS\n",
      "\"OHL\" >> PLAIN = LETTERS\n",
      "\"ATI\" >> LETTERS = PLAIN\n",
      "\"Nham\" >> LETTERS = PLAIN\n",
      "\"I\" >> ORDINAL = PLAIN\n",
      "\"GIS\" >> LETTERS = PLAIN\n",
      "\"21 August\" >> DATE = MEASURE\n",
      "\"1.7 million\" >> DECIMAL = MEASURE\n",
      "\"LOA\" >> PLAIN = LETTERS\n",
      "\"D\" >> PLAIN = LETTERS\n",
      "\"FISD\" >> LETTERS = PLAIN\n",
      "\"th\" >> VERBATIM = PLAIN\n",
      "\"Mpa\" >> LETTERS = PLAIN\n",
      "\"CPL\" >> PLAIN = LETTERS\n",
      "\"SUV\" >> LETTERS = PLAIN\n",
      "\"4\" >> DIGIT = CARDINAL\n",
      "\"Ens\" >> LETTERS = PLAIN\n",
      "\"ahu\" >> LETTERS = PLAIN\n",
      "\"Soc\" >> LETTERS = PLAIN\n",
      "\"MODOK\" >> LETTERS = PLAIN\n",
      "\"D\" >> PLAIN = LETTERS\n",
      "\"192 1067-8\" >> TELEPHONE = MEASURE\n",
      "\":\" >> PLAIN = PUNCT\n",
      "\"G\" >> PLAIN = LETTERS\n",
      "\"-\" >> PUNCT = PLAIN\n",
      "\"xx\" >> VERBATIM = PLAIN\n",
      "\"aka\" >> LETTERS = PLAIN\n",
      "\"GUS\" >> PLAIN = LETTERS\n",
      "\"-\" >> VERBATIM = PLAIN\n",
      "\"TERYT\" >> LETTERS = PLAIN\n",
      "\"1665\" >> DIGIT = DATE\n",
      "\"D\" >> PLAIN = LETTERS\n",
      "\"X\" >> ORDINAL = PLAIN\n",
      "\"X\" >> ORDINAL = PLAIN\n",
      "\"-\" >> VERBATIM = PLAIN\n",
      "\"2968\" >> CARDINAL = DATE\n",
      "\"1,000ft\" >> MEASURE = ORDINAL\n",
      "\"1967\" >> DIGIT = DATE\n",
      "\"Joep\" >> LETTERS = PLAIN\n",
      "\"ITA\" >> PLAIN = LETTERS\n",
      "\"-\" >> VERBATIM = PLAIN\n",
      "\"31 January\" >> DATE = MEASURE\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "for error in errors[:100]:\n",
    "    sys.stdout.write('\"{0}\" >> {1} = {2}\\n'.format(error[0].encode('utf-8'), error[1], error[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./lc_classifier/best.chkp\n",
      "\"ALCS\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"-\" >> VERBATIM = PLAIN VERBATIM PUNCT\n",
      "\"LL\" >> PLAIN = LETTERS PLAIN VERBATIM\n",
      "\"Aceh\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"Aceh\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"Lviv\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"Ukh\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"FRELIMO\" >> PLAIN = LETTERS PLAIN VERBATIM\n",
      "\"POMZ\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"Boev\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"-\" >> VERBATIM = PLAIN VERBATIM PUNCT\n",
      "\"25-08-2015\" >> DATE = TELEPHONE DATE MEASURE\n",
      "\"POLARCAT\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"M\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"DoD\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"Revd\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"VIDEO\" >> PLAIN = LETTERS PLAIN ORDINAL\n",
      "\"-\" >> VERBATIM = PLAIN VERBATIM PUNCT\n",
      "\"OL\" >> LETTERS = PLAIN LETTERS ORDINAL\n",
      "\"MUV\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"SUV\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"UA\" >> PLAIN = LETTERS VERBATIM PLAIN\n",
      "\"CO\" >> LETTERS = VERBATIM LETTERS PLAIN\n",
      "\"DOH\" >> PLAIN = LETTERS PLAIN VERBATIM\n",
      "\"SHU\" >> PLAIN = LETTERS PLAIN VERBATIM\n",
      "\"F\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"200\" >> DIGIT = CARDINAL DIGIT DATE\n",
      "\"X\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"PIALAT\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"GETS\" >> PLAIN = LETTERS PLAIN VERBATIM\n",
      "\"b\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"8 a.m.\" >> TIME = DATE TIME TELEPHONE\n",
      "\"XIX\" >> CARDINAL = PLAIN LETTERS VERBATIM\n",
      "\"G\" >> PLAIN = LETTERS PLAIN VERBATIM\n",
      "\"-\" >> VERBATIM = PLAIN VERBATIM PUNCT\n",
      "\"-\" >> VERBATIM = PLAIN VERBATIM PUNCT\n",
      "\"Wim\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"INTELLIGENCE\" >> PLAIN = LETTERS PLAIN MONEY\n",
      "\"-\" >> VERBATIM = PLAIN VERBATIM PUNCT\n",
      "\"747\" >> DIGIT = CARDINAL DIGIT DATE\n",
      "\"T\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"1/2007\" >> FRACTION = DATE TIME TELEPHONE\n",
      "\"1917\" >> CARDINAL = DATE CARDINAL DIGIT\n",
      "\"-\" >> PUNCT = PLAIN VERBATIM PUNCT\n",
      "\"1918\" >> CARDINAL = DATE CARDINAL DIGIT\n",
      "\"Wim\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"K\" >> PLAIN = VERBATIM PLAIN LETTERS\n",
      "\"K\" >> PLAIN = VERBATIM PLAIN LETTERS\n",
      "\"seq\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"Pnyx\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"DAFIF\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"δ\" >> VERBATIM = PLAIN LETTERS VERBATIM\n",
      "\"I\" >> CARDINAL = PLAIN CARDINAL LETTERS\n",
      "\"2012\" >> CARDINAL = DATE CARDINAL DIGIT\n",
      "\"-\" >> PUNCT = PLAIN VERBATIM PUNCT\n",
      "\"V.\" >> ORDINAL = LETTERS ORDINAL CARDINAL\n",
      "\"-\" >> VERBATIM = PLAIN VERBATIM PUNCT\n",
      "\"UMIST\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"GRIN\" >> PLAIN = LETTERS PLAIN VERBATIM\n",
      "\"OHL\" >> PLAIN = LETTERS PLAIN VERBATIM\n",
      "\"ATI\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"Nham\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"I\" >> ORDINAL = PLAIN CARDINAL LETTERS\n",
      "\"GIS\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"21 August\" >> DATE = MEASURE DATE DECIMAL\n",
      "\"1.7 million\" >> DECIMAL = MEASURE DECIMAL ELECTRONIC\n",
      "\"LOA\" >> PLAIN = LETTERS PLAIN VERBATIM\n",
      "\"D\" >> PLAIN = LETTERS PLAIN VERBATIM\n",
      "\"FISD\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"th\" >> VERBATIM = PLAIN LETTERS VERBATIM\n",
      "\"Mpa\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"CPL\" >> PLAIN = LETTERS PLAIN VERBATIM\n",
      "\"SUV\" >> LETTERS = PLAIN LETTERS VERBATIM\n",
      "\"4\" >> DIGIT = CARDINAL DIGIT VERBATIM\n"
     ]
    }
   ],
   "source": [
    "validation_data = train['class'].values\n",
    "input_data = train['before'].values\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "model = LabelClassifier.load_params('./lc_model.json')\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, './lc_classifier/best.chkp')\n",
    "\n",
    "\n",
    "result = model.generate(sess, input_data[:10000])\n",
    "errors = []\n",
    "for i in range(len(result)):\n",
    "    if validation_data[i] != result[i][0]:\n",
    "        print '\"{0}\" >> {1} = {2} {3} {4}'.format(input_data[i].encode('utf-8'), validation_data[i], result[i][0], result[i][1], result[i][2])\n",
    "        errors.append((input_data[i], validation_data[i], result[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print len(errors)\n",
    "\n",
    "for error in errors[:100]:\n",
    "    print '\"{0}\" >> {1} = {2} {3} {4}\\n'.format(error[0].encode('utf-8'), error[1], error[2][0], error[2][2], error[2][2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
